{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_by_url(url):\n",
    "    \n",
    "    html_ = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_, 'lxml')\n",
    "    \n",
    "    return soup    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем номер последней страницы\n",
    "def get_number_last_page():\n",
    "    \n",
    "    soup = get_soup_by_url('https://www.tomsk.ru09.ru/realty?type=1&otype=1&district[1]=on&district[2]=on&district[3]=on&district[4]=on&perpage=50&page=1')\n",
    "    number_last_page = int(soup.find('td', {'class':'pager_pages'}).find_all('a')[4].text)\n",
    "    \n",
    "    return number_last_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_district_field(keys):\n",
    "    \n",
    "    for i, j in enumerate(keys):\n",
    "        if 'район' in j:\n",
    "            break\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_apartment(url):\n",
    "    \n",
    "    soup = get_soup_by_url(url)\n",
    "    \n",
    "    keys = [i.find('span').text.replace('\\xa0','').lower() for i in soup.find_all('tr', {'class': 'realty_detail_attr'})]\n",
    "    \n",
    "    district_idx = find_district_field(keys)\n",
    "    items = {'район': keys[district_idx]}\n",
    "\n",
    "    keys = [j for i, j in enumerate(keys) if i not in (district_idx - 1, district_idx)]\n",
    "    values = [i.text.replace('\\xa0', ' ') for i in soup.find_all(class_='nowrap')]\n",
    "    \n",
    "    items.update(dict(zip(keys, values)))\n",
    "    items['адрес'] = soup.find(class_='table_map_link').text.replace('\\xa0', ' ')\n",
    "    items['цена'] = int(soup.find('div', {'class': 'realty_detail_price inline'}).text.replace('\\xa0','').replace('руб.',''))\n",
    "    items['ид'] = int(soup.find('strong').text)\n",
    "    items['дата добавления'] = soup.find(class_='realty_detail_date nobr').get('title')\n",
    "    items['дата истечения'] = soup.find_all(class_='realty_detail_date')[4].get('title')\n",
    "    \n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_dataframe(df):\n",
    "    \n",
    "    some_keys = ['общая площадь', 'жилая','кухня']\n",
    "    for i in some_keys:\n",
    "        df[i] = pd.to_numeric([i.split(' ')[0] if not isinstance(i, float) else i for i in df[i].values])\n",
    "        \n",
    "    df['дата добавления'] = pd.to_datetime(df['дата добавления'], format='%d.%m.%Y %H:%M:%S')\n",
    "    df['дата истечения'] = pd.to_datetime(df['дата истечения'], format='%d.%m.%Y')\n",
    "    df['этаж'] = [int(i[0]) if i[0].isdigit() else 0 for i in df['этаж/этажность'].str.split('/')]\n",
    "    df.drop('этаж/этажность', axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_pages(start_page=1, end_page=None):\n",
    "    \n",
    "    url_base = 'https://www.tomsk.ru09.ru/realty?type=1&otype=1&district[1]=on&district[2]=on&district[3]=on&district[4]=on&perpage=50&page='\n",
    "    \n",
    "    end_page = end_page or get_number_last_page()\n",
    "    pages_to_parse = range(start_page, end_page + 1)\n",
    "    urls_pages = [url_base + str(i) for i in pages_to_parse]\n",
    "        \n",
    "    return urls_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_apartments_by_page(url_page):\n",
    "    \n",
    "    url_base = 'https://www.tomsk.ru09.ru'\n",
    "    \n",
    "    soup = get_soup_by_url(url_page)\n",
    "    soup = soup.find_all('a', {'class':'visited_ads'})\n",
    "\n",
    "    urls_apartments = set()\n",
    "    \n",
    "    for i in soup:\n",
    "        urls_apartments.add(url_base + i.get('href'))\n",
    "    \n",
    "    return urls_apartments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(start_page=1, end_page=None, filename='data.json'):\n",
    "    \n",
    "    urls_pages = get_urls_pages(start_page, end_page)\n",
    "    path = 'C:/Users/qwerty.Oleg/'\n",
    "    \n",
    "    if filename in listdir(path):\n",
    "        with open(filename, 'r') as fp:\n",
    "            storage_dict = json.load(fp)\n",
    "        len_storage = len(storage_dict)\n",
    "        print('Apartments in storage: {}\\n'.format(len_storage))\n",
    "    else:\n",
    "        storage_dict = {}\n",
    "        \n",
    "    for url_page in tqdm_notebook(urls_pages, desc='Pages:'):\n",
    "        urls_apartments = get_urls_apartments_by_page(url_page)\n",
    "        urls_apartments_to_parse = urls_apartments.difference(set(storage_dict))\n",
    "        \n",
    "        if len(urls_apartments_to_parse) != 0:\n",
    "            for url_apartment in tqdm_notebook(urls_apartments_to_parse, desc='Apartments:', leave=False):\n",
    "                storage_dict[url_apartment] = parse_apartment(url_apartment)\n",
    "                \n",
    "        with open(filename, 'w') as fp:\n",
    "            json.dump(storage_dict, fp)\n",
    "    print('New apartments: {}'.format(len(storage_dict)-len_storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data.json', orient='index')\n",
    "df = handle_dataframe(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 606.903818,
   "position": {
    "height": "198.878px",
    "left": "1434.43px",
    "right": "20px",
    "top": "109.977px",
    "width": "235.682px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
